name = "swebench_green_agent"
description = "Assessment agent for SWE-bench software engineering benchmark. Evaluates code patches from white agents against real test suites using Docker-based execution."
version = "1.0.0"

defaultInputModes = ["text"]
defaultOutputModes = ["text"]

[capabilities]
streaming = false

[[skills]]
id = "host_assess_swebench"
name = "SWE-bench assessment hosting"
description = """
Assess the software engineering ability of an agent by evaluating patches against the SWE-bench benchmark.
The green agent sends problem statements to white agents, receives patches, and evaluates them using Docker-based test execution.

Datasets available:
- SWE-bench Lite: 300 tasks
- SWE-bench Verified: 500 tasks
- SWE-bench Full: 2294 tasks

Note: Tasks are evaluated sequentially with optional concurrency (default: 4 workers). Full dataset evaluation takes many hours.
"""
tags = ["green agent", "assessment hosting", "swe-bench", "software engineering", "patch evaluation", "batch evaluation"]
examples = ["""
Your task is to run SWE-bench evaluation for the agent located at:
<white_agent_url>
http://localhost:9002/
</white_agent_url>
You should use the following task configuration:
<task_config>
{
  "dataset": "verified",
  "task_ids": null,
  "timeout": 600,
  "max_workers": 8
}
</task_config>
"""]
